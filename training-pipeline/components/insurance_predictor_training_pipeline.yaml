# PIPELINE DEFINITION
# Name: insurance-prdictor-training-pipeline
# Inputs:
#    data_bucket: str
#    dataset_filename: str
#    git_repo: str
#    model_repo: str
#    project_id: str
components:
  comp-commit-github:
    executorLabel: exec-commit-github
    inputDefinitions:
      parameters:
        git_repo:
          parameterType: STRING
        target_file:
          parameterType: STRING
        user_password:
          parameterType: STRING
  comp-commit-github-2:
    executorLabel: exec-commit-github-2
    inputDefinitions:
      parameters:
        git_repo:
          parameterType: STRING
        target_file:
          parameterType: STRING
        user_password:
          parameterType: STRING
  comp-commit-github-3:
    executorLabel: exec-commit-github-3
    inputDefinitions:
      parameters:
        git_repo:
          parameterType: STRING
        target_file:
          parameterType: STRING
        user_password:
          parameterType: STRING
  comp-compare-models:
    executorLabel: exec-compare-models
    inputDefinitions:
      parameters:
        gr_metrics:
          parameterType: STRUCT
        lr_metrics:
          parameterType: STRUCT
        rf_metrics:
          parameterType: STRUCT
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-condition-1:
    dag:
      tasks:
        commit-github:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-commit-github
          dependentTasks:
          - get-git-password-user
          inputs:
            parameters:
              git_repo:
                componentInputParameter: pipelinechannel--git_repo
              target_file:
                runtimeValue:
                  constant: synchronizer/model_upload.txt
              user_password:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: get-git-password-user
          taskInfo:
            name: commit-github
        get-git-password-user:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-get-git-password-user
          dependentTasks:
          - upload-model-to-gcs
          inputs:
            parameters:
              project_id:
                componentInputParameter: pipelinechannel--project_id
          taskInfo:
            name: get-git-password-user
        upload-model-to-gcs:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-upload-model-to-gcs
          inputs:
            artifacts:
              model:
                componentInputArtifact: pipelinechannel--train-lr-out_model
            parameters:
              model_repo:
                componentInputParameter: pipelinechannel--model_repo
              project_id:
                componentInputParameter: pipelinechannel--project_id
          taskInfo:
            name: upload-model-to-gcs
    inputDefinitions:
      artifacts:
        pipelinechannel--train-lr-out_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--compare-models-Output:
          parameterType: STRING
        pipelinechannel--git_repo:
          parameterType: STRING
        pipelinechannel--model_repo:
          parameterType: STRING
        pipelinechannel--project_id:
          parameterType: STRING
  comp-condition-2:
    dag:
      tasks:
        commit-github-2:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-commit-github-2
          dependentTasks:
          - get-git-password-user-2
          inputs:
            parameters:
              git_repo:
                componentInputParameter: pipelinechannel--git_repo
              target_file:
                runtimeValue:
                  constant: synchronizer/model_upload.txt
              user_password:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: get-git-password-user-2
          taskInfo:
            name: commit-github-2
        get-git-password-user-2:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-get-git-password-user-2
          dependentTasks:
          - upload-model-to-gcs-2
          inputs:
            parameters:
              project_id:
                componentInputParameter: pipelinechannel--project_id
          taskInfo:
            name: get-git-password-user-2
        upload-model-to-gcs-2:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-upload-model-to-gcs-2
          inputs:
            artifacts:
              model:
                componentInputArtifact: pipelinechannel--train-rf-out_model
            parameters:
              model_repo:
                componentInputParameter: pipelinechannel--model_repo
              project_id:
                componentInputParameter: pipelinechannel--project_id
          taskInfo:
            name: upload-model-to-gcs-2
    inputDefinitions:
      artifacts:
        pipelinechannel--train-rf-out_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--compare-models-Output:
          parameterType: STRING
        pipelinechannel--git_repo:
          parameterType: STRING
        pipelinechannel--model_repo:
          parameterType: STRING
        pipelinechannel--project_id:
          parameterType: STRING
  comp-condition-3:
    dag:
      tasks:
        commit-github-3:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-commit-github-3
          dependentTasks:
          - get-git-password-user-3
          inputs:
            parameters:
              git_repo:
                componentInputParameter: pipelinechannel--git_repo
              target_file:
                runtimeValue:
                  constant: synchronizer/model_upload.txt
              user_password:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: get-git-password-user-3
          taskInfo:
            name: commit-github-3
        get-git-password-user-3:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-get-git-password-user-3
          dependentTasks:
          - upload-model-to-gcs-3
          inputs:
            parameters:
              project_id:
                componentInputParameter: pipelinechannel--project_id
          taskInfo:
            name: get-git-password-user-3
        upload-model-to-gcs-3:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-upload-model-to-gcs-3
          inputs:
            artifacts:
              model:
                componentInputArtifact: pipelinechannel--train-gr-out_model
            parameters:
              model_repo:
                componentInputParameter: pipelinechannel--model_repo
              project_id:
                componentInputParameter: pipelinechannel--project_id
          taskInfo:
            name: upload-model-to-gcs-3
    inputDefinitions:
      artifacts:
        pipelinechannel--train-gr-out_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--compare-models-Output:
          parameterType: STRING
        pipelinechannel--git_repo:
          parameterType: STRING
        pipelinechannel--model_repo:
          parameterType: STRING
        pipelinechannel--project_id:
          parameterType: STRING
  comp-download-data:
    executorLabel: exec-download-data
    inputDefinitions:
      parameters:
        bucket:
          parameterType: STRING
        file_name:
          parameterType: STRING
        project_id:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-get-git-password-user:
    executorLabel: exec-get-git-password-user
    inputDefinitions:
      parameters:
        project_id:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-get-git-password-user-2:
    executorLabel: exec-get-git-password-user-2
    inputDefinitions:
      parameters:
        project_id:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-get-git-password-user-3:
    executorLabel: exec-get-git-password-user-3
    inputDefinitions:
      parameters:
        project_id:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-train-gr:
    executorLabel: exec-train-gr
    inputDefinitions:
      artifacts:
        X_test:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        X_train:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        Y_test:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        Y_train:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        out_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        metrics:
          parameterType: STRUCT
  comp-train-lr:
    executorLabel: exec-train-lr
    inputDefinitions:
      artifacts:
        X_test:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        X_train:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        Y_test:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        Y_train:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        out_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        metrics:
          parameterType: STRUCT
  comp-train-rf:
    executorLabel: exec-train-rf
    inputDefinitions:
      artifacts:
        X_test:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        X_train:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        Y_test:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        Y_train:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        out_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        metrics:
          parameterType: STRUCT
  comp-train-test-split:
    executorLabel: exec-train-test-split
    inputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        X_test:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        X_train:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        Y_test:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        Y_train:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-upload-model-to-gcs:
    executorLabel: exec-upload-model-to-gcs
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        model_repo:
          parameterType: STRING
        project_id:
          parameterType: STRING
  comp-upload-model-to-gcs-2:
    executorLabel: exec-upload-model-to-gcs-2
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        model_repo:
          parameterType: STRING
        project_id:
          parameterType: STRING
  comp-upload-model-to-gcs-3:
    executorLabel: exec-upload-model-to-gcs-3
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        model_repo:
          parameterType: STRING
        project_id:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-commit-github:
      container:
        args:
        - '{{$.inputs.parameters[''user_password'']}}'
        - '{{$.inputs.parameters[''git_repo'']}}'
        - '{{$.inputs.parameters[''target_file'']}}'
        command:
        - sh
        - -c
        - "GURL=\"https://$0@$1\"                            && git clone $GURL  \
          \                          && cd DataEngineering                       \
          \     && echo \"model uploaded\" >> $2                            && git\
          \ config --global user.email \"testuser@example.com\"                  \
          \          && git config --global user.name \"Test User\"              \
          \              && git commit -am \"model uploaded\"                    \
          \        && git push $GURL --all\n                            "
        image: alpine/git:2.40.1
    exec-commit-github-2:
      container:
        args:
        - '{{$.inputs.parameters[''user_password'']}}'
        - '{{$.inputs.parameters[''git_repo'']}}'
        - '{{$.inputs.parameters[''target_file'']}}'
        command:
        - sh
        - -c
        - "GURL=\"https://$0@$1\"                            && git clone $GURL  \
          \                          && cd DataEngineering                       \
          \     && echo \"model uploaded\" >> $2                            && git\
          \ config --global user.email \"testuser@example.com\"                  \
          \          && git config --global user.name \"Test User\"              \
          \              && git commit -am \"model uploaded\"                    \
          \        && git push $GURL --all\n                            "
        image: alpine/git:2.40.1
    exec-commit-github-3:
      container:
        args:
        - '{{$.inputs.parameters[''user_password'']}}'
        - '{{$.inputs.parameters[''git_repo'']}}'
        - '{{$.inputs.parameters[''target_file'']}}'
        command:
        - sh
        - -c
        - "GURL=\"https://$0@$1\"                            && git clone $GURL  \
          \                          && cd DataEngineering                       \
          \     && echo \"model uploaded\" >> $2                            && git\
          \ config --global user.email \"testuser@example.com\"                  \
          \          && git config --global user.name \"Test User\"              \
          \              && git commit -am \"model uploaded\"                    \
          \        && git push $GURL --all\n                            "
        image: alpine/git:2.40.1
    exec-compare-models:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - compare_models
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef compare_models(lr_metrics: dict, rf_metrics: dict, gr_metrics:\
          \ dict) -> str:\n    import logging\n    import json\n    import sys\n \
          \   logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n    logging.info(lr_metrics)\n\
          \    logging.info(rf_metrics)\n    logging.info(gr_metrics)\n    if lr_metrics.get(\"\
          R^2\") > rf_metrics.get(\"R^2\") and lr_metrics.get(\"R^2\") > gr_metrics.get(\"\
          R^2\"):\n        return \"LR\"\n    elif rf_metrics.get(\"R^2\") > gr_metrics.get(\"\
          R^2\"):\n        return \"RF\"\n    else:\n        return \"GR\"\n\n"
        image: python:3.10.7-slim
    exec-download-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - download_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'google-cloud-storage'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef download_data(project_id: str, bucket: str, file_name: str, dataset:\
          \ Output[Dataset]):\n    '''download data'''\n    from google.cloud import\
          \ storage\n    import pandas as pd\n    import logging \n    import sys\n\
          \n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n   \
          \ # Downloaing the file from a google bucket \n    client = storage.Client(project=project_id)\n\
          \    bucket = client.bucket(bucket)\n    blob = bucket.blob(file_name)\n\
          \    blob.download_to_filename(dataset.path + \".json\")\n    logging.info('Downloaded\
          \ Data!')\n\n"
        image: python:3.10.7-slim
    exec-get-git-password-user:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - get_git_password_user
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-secret-manager'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef get_git_password_user(project_id: str) -> str:  \n\n    from\
          \ google.cloud import secretmanager\n    import logging \n    import sys\n\
          \n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n    #\
          \ ID of the secrets.\n    secret_id_user = \"Github_User\"\n    secret_id_pass\
          \ = \"Github_Password\"\n\n    # Create the Secret Manager client.\n   \
          \ client = secretmanager.SecretManagerServiceClient()\n\n    # Build the\
          \ resource name of the secret version.\n    user_resource = f\"projects/{project_id}/secrets/{secret_id_user}/versions/1\"\
          \n\n    # Get the secret version.\n    user_response = client.access_secret_version(request={\"\
          name\": user_resource})    \n   # Get the value of the secret\n    user_payload\
          \ = user_response.payload.data.decode(\"UTF-8\")\n\n    # Build the resource\
          \ name of the secret version.\n    pass_resource = f\"projects/{project_id}/secrets/{secret_id_pass}/versions/1\"\
          \n    # Get the secret version.\n    pass_response = client.access_secret_version(request={\"\
          name\": pass_resource})\n    pass_payload = pass_response.payload.data.decode(\"\
          UTF-8\")\n\n    logging.info('Github credential retrieved!')\n    return\
          \ user_payload + \":\" + pass_payload  # Never print or log this!    \n\n"
        image: python:3.10.7-slim
    exec-get-git-password-user-2:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - get_git_password_user
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-secret-manager'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef get_git_password_user(project_id: str) -> str:  \n\n    from\
          \ google.cloud import secretmanager\n    import logging \n    import sys\n\
          \n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n    #\
          \ ID of the secrets.\n    secret_id_user = \"Github_User\"\n    secret_id_pass\
          \ = \"Github_Password\"\n\n    # Create the Secret Manager client.\n   \
          \ client = secretmanager.SecretManagerServiceClient()\n\n    # Build the\
          \ resource name of the secret version.\n    user_resource = f\"projects/{project_id}/secrets/{secret_id_user}/versions/1\"\
          \n\n    # Get the secret version.\n    user_response = client.access_secret_version(request={\"\
          name\": user_resource})    \n   # Get the value of the secret\n    user_payload\
          \ = user_response.payload.data.decode(\"UTF-8\")\n\n    # Build the resource\
          \ name of the secret version.\n    pass_resource = f\"projects/{project_id}/secrets/{secret_id_pass}/versions/1\"\
          \n    # Get the secret version.\n    pass_response = client.access_secret_version(request={\"\
          name\": pass_resource})\n    pass_payload = pass_response.payload.data.decode(\"\
          UTF-8\")\n\n    logging.info('Github credential retrieved!')\n    return\
          \ user_payload + \":\" + pass_payload  # Never print or log this!    \n\n"
        image: python:3.10.7-slim
    exec-get-git-password-user-3:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - get_git_password_user
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-secret-manager'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef get_git_password_user(project_id: str) -> str:  \n\n    from\
          \ google.cloud import secretmanager\n    import logging \n    import sys\n\
          \n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n    #\
          \ ID of the secrets.\n    secret_id_user = \"Github_User\"\n    secret_id_pass\
          \ = \"Github_Password\"\n\n    # Create the Secret Manager client.\n   \
          \ client = secretmanager.SecretManagerServiceClient()\n\n    # Build the\
          \ resource name of the secret version.\n    user_resource = f\"projects/{project_id}/secrets/{secret_id_user}/versions/1\"\
          \n\n    # Get the secret version.\n    user_response = client.access_secret_version(request={\"\
          name\": user_resource})    \n   # Get the value of the secret\n    user_payload\
          \ = user_response.payload.data.decode(\"UTF-8\")\n\n    # Build the resource\
          \ name of the secret version.\n    pass_resource = f\"projects/{project_id}/secrets/{secret_id_pass}/versions/1\"\
          \n    # Get the secret version.\n    pass_response = client.access_secret_version(request={\"\
          name\": pass_resource})\n    pass_payload = pass_response.payload.data.decode(\"\
          UTF-8\")\n\n    logging.info('Github credential retrieved!')\n    return\
          \ user_payload + \":\" + pass_payload  # Never print or log this!    \n\n"
        image: python:3.10.7-slim
    exec-train-gr:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_gr
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn==1.2.2'\
          \ 'tensorflow' 'h5py' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_gr (X_train: Input[Dataset], Y_train: Input[Dataset], X_test:\
          \ Input[Dataset], Y_test: Input[Dataset], out_model: Output[Model]) -> NamedTuple('outputs',\
          \ metrics=dict):\n    '''train a GBR with default parameters'''\n    import\
          \ pandas as pd\n    from sklearn.ensemble import GradientBoostingRegressor\n\
          \    from sklearn import metrics\n    import json\n    import logging \n\
          \    import sys\n    import os\n    import pickle\n\n    logging.basicConfig(stream=sys.stdout,\
          \ level=logging.INFO)\n\n    df_x_train = pd.read_json(X_train.path + \"\
          .json\") \n    df_y_train = pd.read_json(Y_train.path + \".json\")     \n\
          \    df_x_test = pd.read_json(X_test.path + \".json\") \n    df_y_test =\
          \ pd.read_json(Y_test.path + \".json\")\n\n    # define model\n    model_gr\
          \ = GradientBoostingRegressor()\n\n    # Fit the model\n    model_gr.fit(df_x_train,\
          \ df_y_train)\n\n    y_pred = model_gr.predict(df_x_test)\n    # evaluate\
          \ the model\n    scores_1 = metrics.r2_score(df_y_test, y_pred)\n    scores_2\
          \ = metrics.mean_absolute_error(df_y_test, y_pred)\n    metrics_dict = {\n\
          \        \"R^2\": scores_1,\n        \"Mean absolute error\": scores_2\n\
          \    }\n\n    logging.info(metrics_dict)  \n\n    # Save the model\n   \
          \ m_file = out_model.path\n    with open(m_file, 'wb') as file:  \n    \
          \    pickle.dump(model_gr, file)\n    outputs = NamedTuple('outputs', metrics=dict)\n\
          \    return outputs(metrics_dict)\n\n"
        image: python:3.10.7-slim
    exec-train-lr:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_lr
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn==1.2.2'\
          \ 'tensorflow' 'h5py' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_lr (X_train: Input[Dataset], Y_train: Input[Dataset], X_test:\
          \ Input[Dataset], Y_test: Input[Dataset], out_model: Output[Model]) -> NamedTuple('outputs',\
          \ metrics=dict):\n    '''train a LR with default parameters'''\n    import\
          \ pandas as pd\n    from sklearn.linear_model import LinearRegression\n\
          \    from sklearn import metrics\n    import json\n    import logging \n\
          \    import sys\n    import os\n    import pickle\n\n    logging.basicConfig(stream=sys.stdout,\
          \ level=logging.INFO)\n\n    df_x_train = pd.read_json(X_train.path + \"\
          .json\") \n    df_y_train = pd.read_json(Y_train.path + \".json\")     \n\
          \    df_x_test = pd.read_json(X_test.path + \".json\") \n    df_y_test =\
          \ pd.read_json(Y_test.path + \".json\")\n\n    # define model\n    model_lr\
          \ = LinearRegression()\n\n    # Fit the model\n    model_lr.fit(df_x_train,\
          \ df_y_train)\n\n    y_pred = model_lr.predict(df_x_test)\n    # evaluate\
          \ the model\n    scores_1 = metrics.r2_score(df_y_test, y_pred)\n    scores_2\
          \ = metrics.mean_absolute_error(df_y_test, y_pred)\n    metrics_dict = {\n\
          \        \"R^2\": scores_1,\n        \"Mean absolute error\": scores_2\n\
          \    }\n\n    logging.info(metrics_dict)  \n\n    # Save the model\n   \
          \ m_file = out_model.path\n    with open(m_file, 'wb') as file:  \n    \
          \    pickle.dump(model_lr, file)\n    outputs = NamedTuple('outputs', metrics=dict)\n\
          \    return outputs(metrics_dict)     \n\n"
        image: python:3.10.7-slim
    exec-train-rf:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_rf
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn==1.2.2'\
          \ 'tensorflow' 'h5py' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_rf (X_train: Input[Dataset], Y_train: Input[Dataset], X_test:\
          \ Input[Dataset], Y_test: Input[Dataset], out_model: Output[Model]) -> NamedTuple('outputs',\
          \ metrics=dict):\n    '''train a RFR with default parameters'''\n    import\
          \ pandas as pd\n    from sklearn.ensemble import RandomForestRegressor\n\
          \    from sklearn import metrics\n    import json\n    import logging \n\
          \    import sys\n    import os\n    import pickle  \n\n    logging.basicConfig(stream=sys.stdout,\
          \ level=logging.INFO)\n\n    df_x_train = pd.read_json(X_train.path + \"\
          .json\") \n    df_y_train = pd.read_json(Y_train.path + \".json\")     \n\
          \    df_x_test = pd.read_json(X_test.path + \".json\") \n    df_y_test =\
          \ pd.read_json(Y_test.path + \".json\")\n\n    # define model\n    model_rf\
          \ = RandomForestRegressor()\n\n    # Fit the model\n    model_rf.fit(df_x_train,\
          \ df_y_train)\n\n    y_pred = model_rf.predict(df_x_test)\n    # evaluate\
          \ the model\n    scores_1 = metrics.r2_score(df_y_test, y_pred)\n    scores_2\
          \ = metrics.mean_absolute_error(df_y_test, y_pred)\n    metrics_dict = {\n\
          \        \"R^2\": scores_1,\n        \"Mean absolute error\": scores_2\n\
          \    }\n\n    logging.info(metrics_dict)  \n\n    # Save the model\n   \
          \ m_file = out_model.path\n    with open(m_file, 'wb') as file:  \n    \
          \    pickle.dump(model_rf, file)\n    outputs = NamedTuple('outputs', metrics=dict)\n\
          \    return outputs(metrics_dict)\n\n"
        image: python:3.10.7-slim
    exec-train-test-split:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_test_split
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn==1.2.2'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_test_split(dataset: Input[Dataset], X_train: Output[Dataset],\
          \ X_test: Output[Dataset], Y_train: Output[Dataset], Y_test: Output[Dataset]):\n\
          \    '''train_test_split'''\n    import pandas as pd\n    import logging\
          \ \n    import sys\n    from sklearn.model_selection import train_test_split\n\
          \n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n   \
          \ data = pd.read_json(dataset.path + \".json\", orient='records')\n\n  \
          \  logging.info(data.head())\n\n    data_values = data.values\n\n    X =\
          \ data_values[:, 0:6]\n    Y = data_values[:, 6]\n\n    x_train, x_test,\
          \ y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\
          \n    pd.DataFrame(x_train).to_json(X_train.path + '.json', orient='records')\n\
          \    pd.DataFrame(x_test).to_json(X_test.path + '.json', orient='records')\n\
          \    pd.DataFrame(y_train).to_json(Y_train.path + '.json', orient='records')\n\
          \    pd.DataFrame(y_test).to_json(Y_test.path + '.json', orient='records')\n\
          \n"
        image: python:3.10.7-slim
    exec-upload-model-to-gcs:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - upload_model_to_gcs
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-storage'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef upload_model_to_gcs(project_id: str, model_repo: str, model:\
          \ Input[Model]):\n    '''upload model to gsc'''\n    from google.cloud import\
          \ storage   \n    import logging \n    import sys\n\n    logging.basicConfig(stream=sys.stdout,\
          \ level=logging.INFO)    \n\n    # upload the model to GCS\n    client =\
          \ storage.Client(project=project_id)\n    bucket = client.bucket(model_repo)\n\
          \    blob = bucket.blob('insurance_model') \n    blob.upload_from_filename(model.path)\
          \       \n\n    logging.info(\"Saved the model to GCP bucket : \" + model_repo)\n\
          \n"
        image: python:3.10.7-slim
    exec-upload-model-to-gcs-2:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - upload_model_to_gcs
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-storage'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef upload_model_to_gcs(project_id: str, model_repo: str, model:\
          \ Input[Model]):\n    '''upload model to gsc'''\n    from google.cloud import\
          \ storage   \n    import logging \n    import sys\n\n    logging.basicConfig(stream=sys.stdout,\
          \ level=logging.INFO)    \n\n    # upload the model to GCS\n    client =\
          \ storage.Client(project=project_id)\n    bucket = client.bucket(model_repo)\n\
          \    blob = bucket.blob('insurance_model') \n    blob.upload_from_filename(model.path)\
          \       \n\n    logging.info(\"Saved the model to GCP bucket : \" + model_repo)\n\
          \n"
        image: python:3.10.7-slim
    exec-upload-model-to-gcs-3:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - upload_model_to_gcs
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.3.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-storage'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef upload_model_to_gcs(project_id: str, model_repo: str, model:\
          \ Input[Model]):\n    '''upload model to gsc'''\n    from google.cloud import\
          \ storage   \n    import logging \n    import sys\n\n    logging.basicConfig(stream=sys.stdout,\
          \ level=logging.INFO)    \n\n    # upload the model to GCS\n    client =\
          \ storage.Client(project=project_id)\n    bucket = client.bucket(model_repo)\n\
          \    blob = bucket.blob('insurance_model') \n    blob.upload_from_filename(model.path)\
          \       \n\n    logging.info(\"Saved the model to GCP bucket : \" + model_repo)\n\
          \n"
        image: python:3.10.7-slim
pipelineInfo:
  name: insurance-prdictor-training-pipeline
root:
  dag:
    tasks:
      compare-models:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-compare-models
        dependentTasks:
        - train-gr
        - train-lr
        - train-rf
        inputs:
          parameters:
            gr_metrics:
              taskOutputParameter:
                outputParameterKey: metrics
                producerTask: train-gr
            lr_metrics:
              taskOutputParameter:
                outputParameterKey: metrics
                producerTask: train-lr
            rf_metrics:
              taskOutputParameter:
                outputParameterKey: metrics
                producerTask: train-rf
        taskInfo:
          name: compare-models
      condition-1:
        componentRef:
          name: comp-condition-1
        dependentTasks:
        - compare-models
        - train-lr
        inputs:
          artifacts:
            pipelinechannel--train-lr-out_model:
              taskOutputArtifact:
                outputArtifactKey: out_model
                producerTask: train-lr
          parameters:
            pipelinechannel--compare-models-Output:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: compare-models
            pipelinechannel--git_repo:
              componentInputParameter: git_repo
            pipelinechannel--model_repo:
              componentInputParameter: model_repo
            pipelinechannel--project_id:
              componentInputParameter: project_id
        taskInfo:
          name: condition-1
        triggerPolicy:
          condition: inputs.parameter_values['pipelinechannel--compare-models-Output']
            == 'LR'
      condition-2:
        componentRef:
          name: comp-condition-2
        dependentTasks:
        - compare-models
        - train-rf
        inputs:
          artifacts:
            pipelinechannel--train-rf-out_model:
              taskOutputArtifact:
                outputArtifactKey: out_model
                producerTask: train-rf
          parameters:
            pipelinechannel--compare-models-Output:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: compare-models
            pipelinechannel--git_repo:
              componentInputParameter: git_repo
            pipelinechannel--model_repo:
              componentInputParameter: model_repo
            pipelinechannel--project_id:
              componentInputParameter: project_id
        taskInfo:
          name: condition-2
        triggerPolicy:
          condition: inputs.parameter_values['pipelinechannel--compare-models-Output']
            == 'RF'
      condition-3:
        componentRef:
          name: comp-condition-3
        dependentTasks:
        - compare-models
        - train-gr
        inputs:
          artifacts:
            pipelinechannel--train-gr-out_model:
              taskOutputArtifact:
                outputArtifactKey: out_model
                producerTask: train-gr
          parameters:
            pipelinechannel--compare-models-Output:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: compare-models
            pipelinechannel--git_repo:
              componentInputParameter: git_repo
            pipelinechannel--model_repo:
              componentInputParameter: model_repo
            pipelinechannel--project_id:
              componentInputParameter: project_id
        taskInfo:
          name: condition-3
        triggerPolicy:
          condition: inputs.parameter_values['pipelinechannel--compare-models-Output']
            == 'GR'
      download-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-download-data
        inputs:
          parameters:
            bucket:
              componentInputParameter: data_bucket
            file_name:
              componentInputParameter: dataset_filename
            project_id:
              componentInputParameter: project_id
        taskInfo:
          name: download-data
      train-gr:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-gr
        dependentTasks:
        - train-test-split
        inputs:
          artifacts:
            X_test:
              taskOutputArtifact:
                outputArtifactKey: X_test
                producerTask: train-test-split
            X_train:
              taskOutputArtifact:
                outputArtifactKey: X_train
                producerTask: train-test-split
            Y_test:
              taskOutputArtifact:
                outputArtifactKey: Y_test
                producerTask: train-test-split
            Y_train:
              taskOutputArtifact:
                outputArtifactKey: Y_train
                producerTask: train-test-split
        taskInfo:
          name: train-gr
      train-lr:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-lr
        dependentTasks:
        - train-test-split
        inputs:
          artifacts:
            X_test:
              taskOutputArtifact:
                outputArtifactKey: X_test
                producerTask: train-test-split
            X_train:
              taskOutputArtifact:
                outputArtifactKey: X_train
                producerTask: train-test-split
            Y_test:
              taskOutputArtifact:
                outputArtifactKey: Y_test
                producerTask: train-test-split
            Y_train:
              taskOutputArtifact:
                outputArtifactKey: Y_train
                producerTask: train-test-split
        taskInfo:
          name: train-lr
      train-rf:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-rf
        dependentTasks:
        - train-test-split
        inputs:
          artifacts:
            X_test:
              taskOutputArtifact:
                outputArtifactKey: X_test
                producerTask: train-test-split
            X_train:
              taskOutputArtifact:
                outputArtifactKey: X_train
                producerTask: train-test-split
            Y_test:
              taskOutputArtifact:
                outputArtifactKey: Y_test
                producerTask: train-test-split
            Y_train:
              taskOutputArtifact:
                outputArtifactKey: Y_train
                producerTask: train-test-split
        taskInfo:
          name: train-rf
      train-test-split:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-test-split
        dependentTasks:
        - download-data
        inputs:
          artifacts:
            dataset:
              taskOutputArtifact:
                outputArtifactKey: dataset
                producerTask: download-data
        taskInfo:
          name: train-test-split
  inputDefinitions:
    parameters:
      data_bucket:
        parameterType: STRING
      dataset_filename:
        parameterType: STRING
      git_repo:
        parameterType: STRING
      model_repo:
        parameterType: STRING
      project_id:
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.3.0
